
kubeflow docs for kserve
https://github.com/kubeflow/manifests/tree/master/contrib/kserve


kserve의 모델 메쉬가 뭐야?

ChatGPT의 말:
ChatGPT
KServe의 모델 메쉬(Model Mesh)는 다수의 머신러닝 모델을 Kubernetes 환경에서 효율적으로 배포하고 관리하기 위한 기능입니다. Model Mesh는 특히 대규모 모델 배포 환경에서 효율적인 리소스 사용과 자동 스케일링을 가능하게 해줍니다.

주요 기능
다중 모델 배포: 모델 메쉬는 동일한 인퍼런스 서버에서 여러 개의 모델을 동시 관리할 수 있도록 해줍니다. 모델을 각각 별도의 서버에 할당하지 않고, 요청에 따라 메모리 안팎으로 로드하여 리소스를 절약합니다.

자동 스케일링: KServe 모델 메쉬는 모델을 메모리 내에 필요한 만큼만 로드하고, 트래픽 패턴에 따라 스케일 업/다운하여 리소스 낭비를 줄입니다. 이를 통해 요청이 많을 때는 모델 인스턴스를 늘리고, 요청이 적을 때는 리소스를 최소화하여 비용 절감이 가능합니다.

온디맨드 모델 로드: 모델 메쉬는 모든 모델을 메모리에 항상 로드하는 것이 아니라, 요청이 있을 때만 해당 모델을 로드합니다. 이로 인해 한정된 메모리 내에서도 많은 모델을 효과적으로 관리할 수 있습니다.

멀티 프레임워크 지원: TensorFlow, PyTorch, ONNX 등 다양한 모델을 지원하여 다양한 ML 프레임워크와 함께 사용할 수 있습니다.

통합 관리 및 모니터링: Model Mesh는 KServe와 통합된 환경에서 중앙 집중형 관리 및 모니터링이 가능해, 클러스터에서 실행되는 모든 모델을 쉽게 관리하고 추적할 수 있습니다.